_cqa_text_report = {
  paths = {
    {
      hint = {
        {
          details = "These instructions generate more than one micro-operation and only one of them can be decoded during a cycle and the extra micro-operations increase pressure on execution units.\n - VGATHERQPS: 12 occurrences\n",
          title = "Complex instructions",
          txt = "Detected COMPLEX INSTRUCTIONS.\n",
        },
        {
          workaround = "Try to remove indirect accesses. If applicable, precompute elements out of the innermost loop.",
          details = " - Irregular (variable stride) or indirect: 3 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Try to simplify your code and/or replace indirect accesses with unit-stride ones.",
          details = " - VGATHERQPS: 12 occurrences\n",
          title = "Gather/scatter instructions",
          txt = "Detected gather/scatter instructions (typically caused by indirect accesses).",
        },
        {
          title = "Type of elements and instruction set",
          txt = "44 SSE or AVX instructions are processing arithmetic or math operations on single precision FP elements in vector mode (four at a time).\n",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop is composed of 272 FP arithmetical operations:\n - 144: addition or subtraction (96 inside FMA instructions)\n - 112: multiply (96 inside FMA instructions)\n - 16: square root\nThe binary loop is loading 356 bytes (89 single precision FP elements).\nThe binary loop is storing 112 bytes (28 single precision FP elements).",
        },
        {
          title = "Arithmetic intensity",
          txt = "Arithmetic intensity is 0.58 FP operations per loaded or stored byte.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 104\nnb uops            : 139\nloop length        : 565\nused x86 registers : 6\nused mmx registers : 0\nused xmm registers : 16\nused ymm registers : 2\nused zmm registers : 0\nnb stack references: 8\nADD-SUB / MUL ratio: 3.00\n",
        },
        {
          title = "Front-end",
          txt = "ASSUMED MACRO FUSION\nFIT IN UOP CACHE\nmicro-operation queue: 34.75 cycles\nfront end            : 34.75 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | P0    | P1    | P2    | P3    | P4   | P5    | P6   | P7\n-------------------------------------------------------------------\nuops   | 40.50 | 40.50 | 28.50 | 28.50 | 6.00 | 13.00 | 2.00 | 6.00\ncycles | 40.50 | 40.50 | 28.50 | 28.50 | 6.00 | 13.00 | 2.00 | 6.00\n\nCycles executing div or sqrt instructions: 12.00\nLongest recurrence chain latency (RecMII): 16.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 34.75\nDispatch  : 40.50\nDIV/SQRT  : 12.00\nData deps.: 16.00\nOverall L1: 40.50\n",
        },
        {
          title = "Vectorization ratios",
          txt = "INT\nall    : 100%\nload   : 100%\nstore  : 100%\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: 100%\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 100%\nFP\nall     : 98%\nload    : 94%\nstore   : 100%\nmul     : 100%\nadd-sub : 100%\nfma     : 100%\ndiv/sqrt: 100%\nother   : 96%\nINT+FP\nall     : 99%\nload    : 95%\nstore   : 100%\nmul     : 100%\nadd-sub : 100%\nfma     : 100%\ndiv/sqrt: 100%\nother   : 97%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "INT\nall    : 61%\nload   : 100%\nstore  : 100%\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: 100%\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 53%\nFP\nall     : 49%\nload    : 48%\nstore   : 50%\nmul     : 50%\nadd-sub : 50%\nfma     : 50%\ndiv/sqrt: 50%\nother   : 48%\nINT+FP\nall     : 51%\nload    : 52%\nstore   : 58%\nmul     : 50%\nadd-sub : 53%\nfma     : 50%\ndiv/sqrt: 50%\nother   : 50%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 40.50 cycles. At this rate:\n - 13% of peak load performance is reached (8.79 out of 64.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 8% of peak store performance is reached (2.77 out of 32.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 4032e0\n\nInstruction                                 | Nb FU | P0   | P1   | P2   | P3   | P4 | P5   | P6   | P7   | Latency | Recip. throughput\n---------------------------------------------------------------------------------------------------------------------------------------\nVMOVDQU %YMM0,0x100(%RSP)                   | 1     | 0    | 0    | 0.33 | 0.33 | 1  | 0    | 0    | 0.33 | 4       | 1\nVPSLLQ $0x2,%YMM0,%YMM12                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVXORPS %XMM9,%XMM9,%XMM9                    | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVPCMPEQD %XMM3,%XMM3,%XMM3                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVGATHERQPS %XMM3,(%RDX,%YMM12,4),%XMM9      | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVXORPS %XMM3,%XMM3,%XMM3                    | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVPCMPEQD %XMM5,%XMM5,%XMM5                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVGATHERQPS %XMM5,(%RSI,%YMM12,4),%XMM3      | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVPXOR %XMM0,%XMM0,%XMM0                     | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVPCMPEQD %XMM6,%XMM6,%XMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVXORPS %XMM5,%XMM5,%XMM5                    | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVGATHERQPS %XMM6,0x4(%RDX,%YMM12,4),%XMM0   | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVMOVUPS %XMM0,0xd0(%RSP)                    | 1     | 0    | 0    | 0.33 | 0.33 | 1  | 0    | 0    | 0.33 | 3       | 1\nVPCMPEQD %XMM6,%XMM6,%XMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVXORPS %XMM0,%XMM0,%XMM0                    | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVGATHERQPS %XMM6,(%RDI,%YMM12,4),%XMM5      | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVPCMPEQD %XMM6,%XMM6,%XMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVGATHERQPS %XMM6,0x4(%RSI,%YMM12,4),%XMM0   | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVMOVUPS %XMM0,0xc0(%RSP)                    | 1     | 0    | 0    | 0.33 | 0.33 | 1  | 0    | 0    | 0.33 | 3       | 1\nVXORPS %XMM0,%XMM0,%XMM0                    | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVPCMPEQD %XMM6,%XMM6,%XMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVXORPS %XMM7,%XMM7,%XMM7                    | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVGATHERQPS %XMM6,0x8(%RDX,%YMM12,4),%XMM0   | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVMOVUPS %XMM0,0xb0(%RSP)                    | 1     | 0    | 0    | 0.33 | 0.33 | 1  | 0    | 0    | 0.33 | 3       | 1\nVPCMPEQD %XMM6,%XMM6,%XMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVXORPS %XMM0,%XMM0,%XMM0                    | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVGATHERQPS %XMM6,0x4(%RDI,%YMM12,4),%XMM7   | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVMOVUPS %XMM7,0x90(%RSP)                    | 1     | 0    | 0    | 0.33 | 0.33 | 1  | 0    | 0    | 0.33 | 3       | 1\nVPCMPEQD %XMM6,%XMM6,%XMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVGATHERQPS %XMM6,0x8(%RSI,%YMM12,4),%XMM0   | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVMOVUPS %XMM0,0xa0(%RSP)                    | 1     | 0    | 0    | 0.33 | 0.33 | 1  | 0    | 0    | 0.33 | 3       | 1\nVXORPS %XMM14,%XMM14,%XMM14                 | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVXORPS %XMM13,%XMM13,%XMM13                 | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVPCMPEQD %XMM15,%XMM15,%XMM15               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVPCMPEQD %XMM0,%XMM0,%XMM0                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVGATHERQPS %XMM15,0xc(%RDX,%YMM12,4),%XMM13 | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVXORPS %XMM15,%XMM15,%XMM15                 | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVPCMPEQD %XMM7,%XMM7,%XMM7                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVGATHERQPS %XMM0,0x8(%RDI,%YMM12,4),%XMM14  | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVGATHERQPS %XMM7,0xc(%RSI,%YMM12,4),%XMM15  | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVXORPS %XMM11,%XMM11,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVPCMPEQD %XMM7,%XMM7,%XMM7                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVGATHERQPS %XMM7,0xc(%RDI,%YMM12,4),%XMM11  | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVSUBPS %XMM8,%XMM9,%XMM12                   | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVUPS 0x70(%RSP),%XMM9                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 4-5     | 0.50\nVSUBPS %XMM9,%XMM3,%XMM3                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSUBPS %XMM10,%XMM5,%XMM0                   | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVAPS %XMM12,%XMM7                        | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVBROADCASTSS 0xbc18(%RIP),%XMM6             | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD213PS %XMM6,%XMM12,%XMM7              | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM3,%XMM3,%XMM7               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM0,%XMM0,%XMM7               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSQRTPS %XMM7,%XMM5                         | 1     | 1    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 12      | 3\nVMULPS %XMM7,%XMM5,%XMM5                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM12,%XMM5,%XMM2              | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM3,%XMM5,%XMM1               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM0,%XMM5,%XMM4               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVUPS 0xd0(%RSP),%XMM0                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 4-5     | 0.50\nVSUBPS %XMM8,%XMM0,%XMM7                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVUPS 0xc0(%RSP),%XMM0                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 4-5     | 0.50\nVSUBPS %XMM9,%XMM0,%XMM5                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVUPS 0x90(%RSP),%XMM0                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 4-5     | 0.50\nVSUBPS %XMM10,%XMM0,%XMM0                   | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVAPS %XMM7,%XMM3                         | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVFMADD213PS %XMM6,%XMM7,%XMM3               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVAPS %XMM6,%XMM12                        | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVFMADD231PS %XMM5,%XMM5,%XMM3               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM0,%XMM0,%XMM3               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSQRTPS %XMM3,%XMM6                         | 1     | 1    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 12      | 3\nVMULPS %XMM3,%XMM6,%XMM3                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM7,%XMM3,%XMM2               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM5,%XMM3,%XMM1               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVUPS 0xb0(%RSP),%XMM5                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 4-5     | 0.50\nVSUBPS %XMM8,%XMM5,%XMM5                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM0,%XMM3,%XMM4               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVUPS 0xa0(%RSP),%XMM0                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 4-5     | 0.50\nVSUBPS %XMM9,%XMM0,%XMM3                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSUBPS %XMM10,%XMM14,%XMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVAPS %XMM5,%XMM7                         | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVFMADD213PS %XMM12,%XMM5,%XMM7              | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM3,%XMM3,%XMM7               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM6,%XMM6,%XMM7               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSQRTPS %XMM7,%XMM0                         | 1     | 1    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 12      | 3\nVMULPS %XMM7,%XMM0,%XMM0                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM5,%XMM0,%XMM2               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM3,%XMM0,%XMM1               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSUBPS %XMM8,%XMM13,%XMM3                   | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSUBPS %XMM9,%XMM15,%XMM5                   | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM6,%XMM0,%XMM4               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSUBPS %XMM10,%XMM11,%XMM0                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVAPS %XMM3,%XMM6                         | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVFMADD213PS %XMM12,%XMM3,%XMM6              | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM5,%XMM5,%XMM6               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM0,%XMM0,%XMM6               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSQRTPS %XMM6,%XMM7                         | 1     | 1    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 12      | 3\nVMULPS %XMM6,%XMM7,%XMM6                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM3,%XMM6,%XMM2               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM5,%XMM6,%XMM1               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM0,%XMM6,%XMM4               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVDQU 0x100(%RSP),%YMM0                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 5       | 0.50\nVPADDQ 0xe0(%RSP),%YMM0,%YMM0               | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 0    | 0    | 1       | 0.50\nADD $0x4,%RBP                               | 1     | 0.25 | 0.25 | 0    | 0    | 0  | 0.25 | 0.25 | 0    | 1       | 0.25\nCMP %RBX,%RBP                               | 1     | 0.25 | 0.25 | 0    | 0    | 0  | 0.25 | 0.25 | 0    | 1       | 0.25\nJB 4032e0 <move_particles.extracted+0x140>  | 1     | 0.50 | 0    | 0    | 0    | 0  | 0    | 0.50 | 0    | 0       | 0.50-1\n",
        },
      },
      header = {
        "20% of peak computational performance is used (6.72 out of 32.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = " - Pass to your compiler a micro-architecture specialization option:\n  * Please read your compiler manual\n - Use vector aligned instructions:\n  1) align your arrays on 32 bytes boundaries\n  2) inform your compiler that your arrays are vector aligned: read your compiler manual.\n",
          details = "99% of SSE/AVX instructions are used in vector version (process two or more data elements in vector registers):\n - 95% of SSE/AVX loads are used in vector version.\n - 97% of SSE/AVX instructions that are not load, store, addition, subtraction nor multiply instructions are used in vector version.\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is highly vectorized.\nOnly 51% of vector register length is used (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 40.50 to 21.26 cycles (1.91x speedup).",
        },
        {
          workaround = " - Reduce the number of division or square root instructions:\n - If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact.\n - Reduce the number of FP add instructions\n - Reduce the number of FP multiply/FMA instructions\n",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by:\n - execution of divide and square root operations (the divide/square root unit is a bottleneck)\n - execution of FP add operations (the FP add unit is a bottleneck)\n - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 40.50 to 34.75 cycles (1.17x speedup).\n",
        },
      },
      potential = {
        {
          workaround = "Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.\nFor instance a + b*c is a valid FMA (MUL then ADD).\nHowever (a+b)* c cannot be translated into an FMA (ADD then MUL).",
          title = "FMA",
          txt = "Detected 96 FMA (fused multiply-add) operations.\nPresence of both ADD/SUB and MUL operations.",
        },
      },
    },
  },
  AVG = {
      hint = {
        {
          details = "These instructions generate more than one micro-operation and only one of them can be decoded during a cycle and the extra micro-operations increase pressure on execution units.\n - VGATHERQPS: 12 occurrences\n",
          title = "Complex instructions",
          txt = "Detected COMPLEX INSTRUCTIONS.\n",
        },
        {
          workaround = "Try to remove indirect accesses. If applicable, precompute elements out of the innermost loop.",
          details = " - Irregular (variable stride) or indirect: 3 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Try to simplify your code and/or replace indirect accesses with unit-stride ones.",
          details = " - VGATHERQPS: 12 occurrences\n",
          title = "Gather/scatter instructions",
          txt = "Detected gather/scatter instructions (typically caused by indirect accesses).",
        },
        {
          title = "Type of elements and instruction set",
          txt = "44 SSE or AVX instructions are processing arithmetic or math operations on single precision FP elements in vector mode (four at a time).\n",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop is composed of 272 FP arithmetical operations:\n - 144: addition or subtraction (96 inside FMA instructions)\n - 112: multiply (96 inside FMA instructions)\n - 16: square root\nThe binary loop is loading 356 bytes (89 single precision FP elements).\nThe binary loop is storing 112 bytes (28 single precision FP elements).",
        },
        {
          title = "Arithmetic intensity",
          txt = "Arithmetic intensity is 0.58 FP operations per loaded or stored byte.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 104\nnb uops            : 139\nloop length        : 565\nused x86 registers : 6\nused mmx registers : 0\nused xmm registers : 16\nused ymm registers : 2\nused zmm registers : 0\nnb stack references: 8\nADD-SUB / MUL ratio: 3.00\n",
        },
        {
          title = "Front-end",
          txt = "ASSUMED MACRO FUSION\nFIT IN UOP CACHE\nmicro-operation queue: 34.75 cycles\nfront end            : 34.75 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | P0    | P1    | P2    | P3    | P4   | P5    | P6   | P7\n-------------------------------------------------------------------\nuops   | 40.50 | 40.50 | 28.50 | 28.50 | 6.00 | 13.00 | 2.00 | 6.00\ncycles | 40.50 | 40.50 | 28.50 | 28.50 | 6.00 | 13.00 | 2.00 | 6.00\n\nCycles executing div or sqrt instructions: 12.00\nLongest recurrence chain latency (RecMII): 16.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 34.75\nDispatch  : 40.50\nDIV/SQRT  : 12.00\nData deps.: 16.00\nOverall L1: 40.50\n",
        },
        {
          title = "Vectorization ratios",
          txt = "INT\nall    : 100%\nload   : 100%\nstore  : 100%\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: 100%\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 100%\nFP\nall     : 98%\nload    : 94%\nstore   : 100%\nmul     : 100%\nadd-sub : 100%\nfma     : 100%\ndiv/sqrt: 100%\nother   : 96%\nINT+FP\nall     : 99%\nload    : 95%\nstore   : 100%\nmul     : 100%\nadd-sub : 100%\nfma     : 100%\ndiv/sqrt: 100%\nother   : 97%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "INT\nall    : 61%\nload   : 100%\nstore  : 100%\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: 100%\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 53%\nFP\nall     : 49%\nload    : 48%\nstore   : 50%\nmul     : 50%\nadd-sub : 50%\nfma     : 50%\ndiv/sqrt: 50%\nother   : 48%\nINT+FP\nall     : 51%\nload    : 52%\nstore   : 58%\nmul     : 50%\nadd-sub : 53%\nfma     : 50%\ndiv/sqrt: 50%\nother   : 50%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 40.50 cycles. At this rate:\n - 13% of peak load performance is reached (8.79 out of 64.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 8% of peak store performance is reached (2.77 out of 32.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 4032e0\n\nInstruction                                 | Nb FU | P0   | P1   | P2   | P3   | P4 | P5   | P6   | P7   | Latency | Recip. throughput\n---------------------------------------------------------------------------------------------------------------------------------------\nVMOVDQU %YMM0,0x100(%RSP)                   | 1     | 0    | 0    | 0.33 | 0.33 | 1  | 0    | 0    | 0.33 | 4       | 1\nVPSLLQ $0x2,%YMM0,%YMM12                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVXORPS %XMM9,%XMM9,%XMM9                    | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVPCMPEQD %XMM3,%XMM3,%XMM3                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVGATHERQPS %XMM3,(%RDX,%YMM12,4),%XMM9      | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVXORPS %XMM3,%XMM3,%XMM3                    | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVPCMPEQD %XMM5,%XMM5,%XMM5                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVGATHERQPS %XMM5,(%RSI,%YMM12,4),%XMM3      | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVPXOR %XMM0,%XMM0,%XMM0                     | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVPCMPEQD %XMM6,%XMM6,%XMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVXORPS %XMM5,%XMM5,%XMM5                    | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVGATHERQPS %XMM6,0x4(%RDX,%YMM12,4),%XMM0   | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVMOVUPS %XMM0,0xd0(%RSP)                    | 1     | 0    | 0    | 0.33 | 0.33 | 1  | 0    | 0    | 0.33 | 3       | 1\nVPCMPEQD %XMM6,%XMM6,%XMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVXORPS %XMM0,%XMM0,%XMM0                    | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVGATHERQPS %XMM6,(%RDI,%YMM12,4),%XMM5      | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVPCMPEQD %XMM6,%XMM6,%XMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVGATHERQPS %XMM6,0x4(%RSI,%YMM12,4),%XMM0   | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVMOVUPS %XMM0,0xc0(%RSP)                    | 1     | 0    | 0    | 0.33 | 0.33 | 1  | 0    | 0    | 0.33 | 3       | 1\nVXORPS %XMM0,%XMM0,%XMM0                    | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVPCMPEQD %XMM6,%XMM6,%XMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVXORPS %XMM7,%XMM7,%XMM7                    | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVGATHERQPS %XMM6,0x8(%RDX,%YMM12,4),%XMM0   | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVMOVUPS %XMM0,0xb0(%RSP)                    | 1     | 0    | 0    | 0.33 | 0.33 | 1  | 0    | 0    | 0.33 | 3       | 1\nVPCMPEQD %XMM6,%XMM6,%XMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVXORPS %XMM0,%XMM0,%XMM0                    | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVGATHERQPS %XMM6,0x4(%RDI,%YMM12,4),%XMM7   | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVMOVUPS %XMM7,0x90(%RSP)                    | 1     | 0    | 0    | 0.33 | 0.33 | 1  | 0    | 0    | 0.33 | 3       | 1\nVPCMPEQD %XMM6,%XMM6,%XMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVGATHERQPS %XMM6,0x8(%RSI,%YMM12,4),%XMM0   | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVMOVUPS %XMM0,0xa0(%RSP)                    | 1     | 0    | 0    | 0.33 | 0.33 | 1  | 0    | 0    | 0.33 | 3       | 1\nVXORPS %XMM14,%XMM14,%XMM14                 | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVXORPS %XMM13,%XMM13,%XMM13                 | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVPCMPEQD %XMM15,%XMM15,%XMM15               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVPCMPEQD %XMM0,%XMM0,%XMM0                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVGATHERQPS %XMM15,0xc(%RDX,%YMM12,4),%XMM13 | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVXORPS %XMM15,%XMM15,%XMM15                 | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVPCMPEQD %XMM7,%XMM7,%XMM7                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVGATHERQPS %XMM0,0x8(%RDI,%YMM12,4),%XMM14  | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVGATHERQPS %XMM7,0xc(%RSI,%YMM12,4),%XMM15  | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVXORPS %XMM11,%XMM11,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVPCMPEQD %XMM7,%XMM7,%XMM7                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 1       | 0.50\nVGATHERQPS %XMM7,0xc(%RDI,%YMM12,4),%XMM11  | 4     | 1    | 1    | 2    | 2    | 0  | 1    | 0    | 0    | 20      | 4\nVSUBPS %XMM8,%XMM9,%XMM12                   | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVUPS 0x70(%RSP),%XMM9                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 4-5     | 0.50\nVSUBPS %XMM9,%XMM3,%XMM3                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSUBPS %XMM10,%XMM5,%XMM0                   | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVAPS %XMM12,%XMM7                        | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVBROADCASTSS 0xbc18(%RIP),%XMM6             | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD213PS %XMM6,%XMM12,%XMM7              | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM3,%XMM3,%XMM7               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM0,%XMM0,%XMM7               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSQRTPS %XMM7,%XMM5                         | 1     | 1    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 12      | 3\nVMULPS %XMM7,%XMM5,%XMM5                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM12,%XMM5,%XMM2              | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM3,%XMM5,%XMM1               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM0,%XMM5,%XMM4               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVUPS 0xd0(%RSP),%XMM0                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 4-5     | 0.50\nVSUBPS %XMM8,%XMM0,%XMM7                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVUPS 0xc0(%RSP),%XMM0                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 4-5     | 0.50\nVSUBPS %XMM9,%XMM0,%XMM5                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVUPS 0x90(%RSP),%XMM0                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 4-5     | 0.50\nVSUBPS %XMM10,%XMM0,%XMM0                   | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVAPS %XMM7,%XMM3                         | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVFMADD213PS %XMM6,%XMM7,%XMM3               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVAPS %XMM6,%XMM12                        | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVFMADD231PS %XMM5,%XMM5,%XMM3               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM0,%XMM0,%XMM3               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSQRTPS %XMM3,%XMM6                         | 1     | 1    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 12      | 3\nVMULPS %XMM3,%XMM6,%XMM3                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM7,%XMM3,%XMM2               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM5,%XMM3,%XMM1               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVUPS 0xb0(%RSP),%XMM5                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 4-5     | 0.50\nVSUBPS %XMM8,%XMM5,%XMM5                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM0,%XMM3,%XMM4               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVUPS 0xa0(%RSP),%XMM0                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 4-5     | 0.50\nVSUBPS %XMM9,%XMM0,%XMM3                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSUBPS %XMM10,%XMM14,%XMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVAPS %XMM5,%XMM7                         | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVFMADD213PS %XMM12,%XMM5,%XMM7              | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM3,%XMM3,%XMM7               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM6,%XMM6,%XMM7               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSQRTPS %XMM7,%XMM0                         | 1     | 1    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 12      | 3\nVMULPS %XMM7,%XMM0,%XMM0                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM5,%XMM0,%XMM2               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM3,%XMM0,%XMM1               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSUBPS %XMM8,%XMM13,%XMM3                   | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSUBPS %XMM9,%XMM15,%XMM5                   | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM6,%XMM0,%XMM4               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSUBPS %XMM10,%XMM11,%XMM0                  | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVAPS %XMM3,%XMM6                         | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 0       | 0.25\nVFMADD213PS %XMM12,%XMM3,%XMM6              | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM5,%XMM5,%XMM6               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM0,%XMM0,%XMM6               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVSQRTPS %XMM6,%XMM7                         | 1     | 1    | 0    | 0    | 0    | 0  | 0    | 0    | 0    | 12      | 3\nVMULPS %XMM6,%XMM7,%XMM6                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM3,%XMM6,%XMM2               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM5,%XMM6,%XMM1               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVFMADD231PS %XMM0,%XMM6,%XMM4               | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 0    | 0    | 4       | 0.50\nVMOVDQU 0x100(%RSP),%YMM0                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 0    | 0    | 5       | 0.50\nVPADDQ 0xe0(%RSP),%YMM0,%YMM0               | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 0    | 0    | 1       | 0.50\nADD $0x4,%RBP                               | 1     | 0.25 | 0.25 | 0    | 0    | 0  | 0.25 | 0.25 | 0    | 1       | 0.25\nCMP %RBX,%RBP                               | 1     | 0.25 | 0.25 | 0    | 0    | 0  | 0.25 | 0.25 | 0    | 1       | 0.25\nJB 4032e0 <move_particles.extracted+0x140>  | 1     | 0.50 | 0    | 0    | 0    | 0  | 0    | 0.50 | 0    | 0       | 0.50-1\n",
        },
      },
      header = {
        "20% of peak computational performance is used (6.72 out of 32.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = " - Pass to your compiler a micro-architecture specialization option:\n  * Please read your compiler manual\n - Use vector aligned instructions:\n  1) align your arrays on 32 bytes boundaries\n  2) inform your compiler that your arrays are vector aligned: read your compiler manual.\n",
          details = "99% of SSE/AVX instructions are used in vector version (process two or more data elements in vector registers):\n - 95% of SSE/AVX loads are used in vector version.\n - 97% of SSE/AVX instructions that are not load, store, addition, subtraction nor multiply instructions are used in vector version.\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is highly vectorized.\nOnly 51% of vector register length is used (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 40.50 to 21.26 cycles (1.91x speedup).",
        },
        {
          workaround = " - Reduce the number of division or square root instructions:\n - If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact.\n - Reduce the number of FP add instructions\n - Reduce the number of FP multiply/FMA instructions\n",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by:\n - execution of divide and square root operations (the divide/square root unit is a bottleneck)\n - execution of FP add operations (the FP add unit is a bottleneck)\n - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 40.50 to 34.75 cycles (1.17x speedup).\n",
        },
      },
      potential = {
        {
          workaround = "Try to change order in which elements are evaluated (using parentheses) in arithmetic expressions containing both ADD/SUB and MUL operations to enable your compiler to generate FMA instructions wherever possible.\nFor instance a + b*c is a valid FMA (MUL then ADD).\nHowever (a+b)* c cannot be translated into an FMA (ADD then MUL).",
          title = "FMA",
          txt = "Detected 96 FMA (fused multiply-add) operations.\nPresence of both ADD/SUB and MUL operations.",
        },
      },
    },
  common = {
    header = {
      "",
    },
    nb_paths = 1,
  },
}
